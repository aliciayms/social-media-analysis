{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13bbdd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/social/bin/python\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys, numpy\n",
    "print(sys.executable)\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b18f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n social python=3.11 -y\n",
    "# conda activate social\n",
    "# conda install -c conda-forge -y numpy pandas pyarrow ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef348aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 2.4.0\n",
      "pandas: 2.3.3\n",
      "pyarrow: 19.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"pyarrow:\", pa.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66943708",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3e168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_data = pd.read_csv('social_media_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ea4f2",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765497d",
   "metadata": {},
   "source": [
    "Lexical and Morphological Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "707b1038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/yoitsal/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/yoitsal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/yoitsal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [(just, ADV), (tried, VERB), (the, DET), (chro...\n",
      "1        [(just, ADV), (saw, VERB), (an, DET), (ad, NOU...\n",
      "2        [(what, PRON), ('s, VERB), (your, PRON), (opin...\n",
      "3        [(bummed, VERB), (out, PRT), (with, ADP), (my,...\n",
      "4        [(just, ADV), (tried, VERB), (the, DET), (coro...\n",
      "                               ...                        \n",
      "11995    [(comparing, VERB), (toyota, NOUN), (camry, NO...\n",
      "11996    [(my, PRON), (two, NUM), (days, NOUN), (review...\n",
      "11997    [(just, ADV), (unboxed, ADJ), (my, PRON), (new...\n",
      "11998    [(comparing, VERB), (toyota, NOUN), (camry, NO...\n",
      "11999    [(just, ADV), (saw, VERB), (an, DET), (ad, NOU...\n",
      "Name: tags, Length: 12000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# It focuses on identifying and processing words (or lexemes) in a text. \n",
    "# Breaks down the input text into individual tokens that are meaningful units of language such as words or phrases.\n",
    "# Tokenization, Part-of-Speech Tagging\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "social_media_data['text_content'] = social_media_data['text_content'].str.lower()\n",
    "tokens = social_media_data['text_content'].apply(lambda x: word_tokenize(str(x)))\n",
    "social_media_data['tokens'] = tokens\n",
    "tags = social_media_data['tokens'].apply(lambda x: pos_tag(x, tagset='universal'))\n",
    "social_media_data['tags'] = tags\n",
    "print(social_media_data['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d06c965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yoitsal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [just, try, chromebook, google, best, purchase...\n",
      "1        [just, saw, ad, microsoft, surface, laptop, sp...\n",
      "2        ['s, opinion, nike, epic, react, promo, food, ...\n",
      "3        [bum, new, diet, pepsi, pepsi, disappoint, qua...\n",
      "4        [just, try, corolla, toyota, absolutely, love,...\n",
      "                               ...                        \n",
      "11995    [compare, toyota, camry, competition, best, pu...\n",
      "11996    [day, review, apple, airpods, pro, highly, rec...\n",
      "11997    [just, unboxed, new, dri-fit, nike, best, purc...\n",
      "11998    [compare, toyota, camry, competition, do, job,...\n",
      "11999    [just, saw, ad, apple, imac, innovationx, have...\n",
      "Name: tokens, Length: 12000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Morphological Analysis - morphemes (smallest unit of word)\n",
    "# Stemming: Reducing words to their root form, chopping off endings (running -> runn)\n",
    "# Lemmatization: Converting words to their base from context, linguistic knowledge (running -> run)\n",
    "# In this case, we will use lemmatization for higher accuracy\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "universal_to_wordnet = {\n",
    "    \"NOUN\": wordnet.NOUN,\n",
    "    \"VERB\": wordnet.VERB,\n",
    "    \"ADV\" : wordnet.ADV,\n",
    "    \"ADJ\" : wordnet.ADJ\n",
    "    }\n",
    "\n",
    "keep_tags = {\"NOUN\", \"VERB\", \"ADV\", \"ADJ\"}\n",
    "\n",
    "def lemmatize_pos_tagged(tags):\n",
    "    return [\n",
    "        lemmatizer.lemmatize(word, universal_to_wordnet.get(tag,wordnet.NOUN))\n",
    "        for word, tag in tags \n",
    "        if tag in keep_tags\n",
    "    ]\n",
    "\n",
    "lemmas = tags.apply(lemmatize_pos_tagged)\n",
    "print(lemmas) \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "social",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
